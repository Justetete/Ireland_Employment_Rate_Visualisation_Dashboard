{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c1eccb",
   "metadata": {},
   "source": [
    "# COMP40610 Information Visualisation Assignment Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377dcda3",
   "metadata": {},
   "source": [
    "## 1. Data Selection\n",
    "\n",
    "本次作业我选择的数据集是从CSO中的High Value Dataset 中的Labour Market Section中选取。作为即将毕业的爱尔兰国际学生，抱持着对于爱尔兰的就业市场的兴趣，想要分析近年来，在不同因素影响下爱尔兰的就业市场形势，分析爱尔兰的劳动力市场变化情况\n",
    "\n",
    "我从[CSO High Value Datasets](https://www.cso.ie/en/statistics/highvaluedatasetshvd)中的Labour Market and Earnings 板块中下载了相关的数据集，具体原始数据包括：\n",
    "\n",
    " - Annual employment rate\n",
    " - Annual percentage of part-time work\n",
    " - Annual unemployment rate\n",
    " - Annual long term unemployment rate\n",
    " - Annual percentage of potential additional labour force\n",
    " - Quarterly employment rate\n",
    " - Quarterly unemployment and long-term unemployment rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c237d7",
   "metadata": {},
   "source": [
    "## 2. Tasks and Question Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a10a5",
   "metadata": {},
   "source": [
    "根据CSO提供的数据集，我想要探讨的问题（5个）包括：\n",
    "1. 对于就业率和失业率在性别问题上的差异化问题，并分析在不同的区域、不同的教育程度或年龄上，是否会有不同的差异趋势\n",
    "2. 由于女性在生育或家庭中的角色问题，可能一部分女性因承担家庭责任而选择放弃全职的工作，而转而进行兼职工作。因此我们同时想要分析，在兼职率中，不同性别的差异化\n",
    "3. 在爱尔兰，不同的教育程度下的教育率，兼职率，失业率，长期失业率和潜在的劳动力分别为多少？\n",
    "\t- 增添性别分类\n",
    "4. 随着时间的变化，爱尔兰的总体就业率和失业率变化趋势,\n",
    "\t- 可以根据不同季度变化体现\n",
    "\t- 或者按照不同年份的变化体现\n",
    "\t- 注意：可以展现covid-19对于就业率的影响\n",
    "5. 不同年龄段之内的就业模式和差率\n",
    "\t- 青年失业率和爱尔兰总体失业率的对比\n",
    "\t- 加入时间的变化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bccba1",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "\n",
    "I want to anaylyse these tasks/questions so I need to conbined all five datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ebd1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required packages\n",
    "#Import package pandas for data analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Import package numpy for numeric computing\n",
    "import numpy as np\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac8486",
   "metadata": {},
   "source": [
    "- **Import datasets**\n",
    "\n",
    "- Delete unneseccery column\n",
    "\n",
    "    We inspect raw dateset and found the 'Unit' column only has one unique value. It is '%' sign to represent the unit of value. \n",
    "    \n",
    "    So I decided to drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67c424c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataframe shape: (6084, 7)\n",
      "\n",
      "Column names: ['Quarter', 'Sex', 'Education Attainment Level', 'Age Group', 'Employment_Rate', 'Unemployment_Rate', 'Long_term_Unemployment_Rate']\n",
      "\n",
      "First few rows:\n",
      "  Quarter         Sex   Education Attainment Level      Age Group  \\\n",
      "0  2019Q1  Both sexes  Less than primary (Level 0)  15 - 24 years   \n",
      "1  2019Q1  Both sexes  Less than primary (Level 0)  20 - 24 years   \n",
      "2  2019Q1  Both sexes  Less than primary (Level 0)  25 - 29 years   \n",
      "3  2019Q1  Both sexes  Less than primary (Level 0)  25 - 54 years   \n",
      "4  2019Q1  Both sexes  Less than primary (Level 0)  30 - 34 years   \n",
      "\n",
      "   Employment_Rate  Unemployment_Rate  Long_term_Unemployment_Rate  \n",
      "0              NaN                NaN                          NaN  \n",
      "1              NaN                NaN                          NaN  \n",
      "2              NaN                NaN                          NaN  \n",
      "3              NaN                NaN                          NaN  \n",
      "4              NaN                NaN                          NaN  \n",
      "\n",
      "Rows with duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define files\n",
    "files = {\n",
    "    '../raw_datasets/QLF50-Quarterly Employment Rate.csv': 'Employment_Rate',\n",
    "    '../raw_datasets/QLF51-Quarterly Unemployment and Long-term Unemployment Rate.csv': None  # 需要特殊处理\n",
    "}\n",
    "\n",
    "# Common key columns for merging\n",
    "key_columns = [\"Quarter\", \"Sex\", \"Education Attainment Level\", \"Age Group\"]\n",
    "\n",
    "quarter_df = None\n",
    "\n",
    "for filename, metric_name in files.items():\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Drop UNIT column\n",
    "    df = df.drop(['UNIT'], axis=1)\n",
    "    \n",
    "    # Convert VALUE to numeric\n",
    "    df['VALUE'] = pd.to_numeric(df['VALUE'], errors='coerce')\n",
    "    \n",
    "    if metric_name:\n",
    "        # For employment rate - simple case\n",
    "        df = df.drop(['Statistic Label'], axis=1)\n",
    "        df = df.rename(columns={'VALUE': metric_name})\n",
    "        df = df.drop_duplicates(subset=key_columns, keep='first')\n",
    "        quarter_df = df\n",
    "    else:\n",
    "        # For unemployment file - need to pivot based on Statistic Label\n",
    "        # Create separate dataframes for each statistic type\n",
    "        unemployment_df = df[df['Statistic Label'] == 'Unemployment rate'].copy()\n",
    "        unemployment_df = unemployment_df.drop(['Statistic Label'], axis=1)\n",
    "        unemployment_df = unemployment_df.rename(columns={'VALUE': 'Unemployment_Rate'})\n",
    "        unemployment_df = unemployment_df.drop_duplicates(subset=key_columns, keep='first')\n",
    "        \n",
    "        long_term_df = df[df['Statistic Label'] == 'Long-term unemployment rate'].copy()\n",
    "        long_term_df = long_term_df.drop(['Statistic Label'], axis=1)\n",
    "        long_term_df = long_term_df.rename(columns={'VALUE': 'Long_term_Unemployment_Rate'})\n",
    "        long_term_df = long_term_df.drop_duplicates(subset=key_columns, keep='first')\n",
    "        \n",
    "        # Merge unemployment rate\n",
    "        quarter_df = quarter_df.merge(unemployment_df, on=key_columns, how='outer')\n",
    "        \n",
    "        # Merge long-term unemployment rate\n",
    "        quarter_df = quarter_df.merge(long_term_df, on=key_columns, how='outer')\n",
    "\n",
    "print(f\"Combined dataframe shape: {quarter_df.shape}\")\n",
    "print(f\"\\nColumn names: {list(quarter_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\\n{quarter_df.head()}\")\n",
    "\n",
    "# Verify no duplicates\n",
    "duplicates = quarter_df.groupby(key_columns).size()\n",
    "print(f\"\\nRows with duplicates: {(duplicates > 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cea279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_df = quarter_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4e229c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "Quarter                           0\n",
      "Sex                               0\n",
      "Education Attainment Level        0\n",
      "Age Group                         0\n",
      "Employment_Rate                2626\n",
      "Unemployment_Rate              5146\n",
      "Long_term_Unemployment_Rate    5694\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values:\")\n",
    "print(quarter_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208f3ad",
   "metadata": {},
   "source": [
    "### Export the combined datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70de478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_df.to_csv('quarterly_employment_unemployment_rate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d69e7b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m files = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m../raw_datasets/ALF01_Annual Employment Rate.csv\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mEmployment_Rate\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m../raw_datasets/ALF03_Annual Unemployment Rate.csv\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mUnemployment_Rate\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m }\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Common key columns for merging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m key_columns = [\u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAge Group\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSex\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mEducation Attainment Level\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNUTS 2 Region\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load and process the first file\u001b[39;00m\n\u001b[32m     11\u001b[39m first_file = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1368\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1311\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/comp40610IV/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/comp40610IV/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/comp40610IV/lib/python3.11/threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/comp40610IV/lib/python3.11/threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define the files and their corresponding metric column names\n",
    "files = {\n",
    "    '../raw_datasets/ALF01_Annual Employment Rate.csv': 'Employment_Rate',\n",
    "    '../raw_datasets/ALF03_Annual Unemployment Rate.csv': 'Unemployment_Rate'\n",
    "}\n",
    "\n",
    "# Common key columns for merging\n",
    "key_columns = ['Year', 'Age Group', 'Sex', 'Education Attainment Level', 'NUTS 2 Region']\n",
    "\n",
    "# Load and process the first file\n",
    "first_file = True\n",
    "combined_df = None\n",
    "\n",
    "for filename, metric_name in files.items():\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Drop unnecessary columns (UNIT and Statistic Label)\n",
    "    df = df.drop(['UNIT', 'Statistic Label'], axis=1)\n",
    "    \n",
    "    # Rename VALUE column to the metric name\n",
    "    df = df.rename(columns={'VALUE': metric_name})\n",
    "    \n",
    "    # Convert VALUE to numeric, handling empty strings as NaN\n",
    "    df[metric_name] = pd.to_numeric(df[metric_name], errors='coerce')\n",
    "    \n",
    "    if first_file:\n",
    "        # For the first file, use it as the base\n",
    "        combined_df = df\n",
    "        first_file = False\n",
    "    else:\n",
    "        # Merge subsequent files on the key columns\n",
    "        combined_df = combined_df.merge(df, on=key_columns, how='outer')\n",
    "\n",
    "print(f\"Combined dataframe shape: {combined_df.shape}\")\n",
    "print(f\"\\nColumn names: {list(combined_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e59505",
   "metadata": {},
   "source": [
    "- **Eliminate duplicated rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a2a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fce388",
   "metadata": {},
   "source": [
    "- Deal with Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "851c54fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "Quarter                          0\n",
      "Sex                              0\n",
      "Education Attainment Level       0\n",
      "Age Group                        0\n",
      "Employment_Rate               2981\n",
      "Unemployment_Rate             5694\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values:\")\n",
    "print(combined_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb41753",
   "metadata": {},
   "source": [
    "## 4.  Save cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9647f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('combined_clean_data_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268591ec",
   "metadata": {},
   "source": [
    "## Refine\n",
    "\n",
    "Based on the first try on the data visualisation. I think there are some steps to refine or adjust the problem.\n",
    "- [ ]  可以将教育程度的单位重新修改成数字型，因为现在字符太长\n",
    "- [ ] 关于"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp40610IV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
